$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json
type: command

# General information about the component
name: diarize_inference
display_name: Diarization inference
description: A template component to run inference on Multi-Scale Diarization Decoder model by Nvidia.
tags:
  author: Antonio Zarauz Moreno
  version: '1.0'

# Inputs and outputs
inputs:
  input_path:
    type: uri_folder
    optional: false
  input_asr_path:
    type: uri_folder
    optional: false
  event_type:
    type: string
    enum: ['meeting', 'telephonic']
    default: 'telephonic'
    optional: true
  max_num_speakers:
    type: integer
    default: 2
    min: 1
    max: 8
    optional: true
outputs:
  output_path:
    type: uri_folder

# The source code path of it's defined in the code section and when the
# component is run in cloud, all files from that path will be uploaded
# as the snapshot of this component
code: ./

# Environment takes care of source image and dependencies
# https://learn.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-v2?view=azureml-api-2&tabs=cli
environment:
  image: diar_env:1

# Cluster instance
compute: azureml:gpu-cluster

# Distribution type
distribution:
  type: pytorch
  process_count_per_instance: 1 # Number of nodes per instance

# How many VMs we need
resources:
  instance_count: 1 # Number of instances to create

# The command section specifies the command to execute while running
# this component
# When the input is set as optional = true, you need use $[[]] to embrace
# the command line with inputs.
command: python ./main.py --input_path ${{inputs.input_path}} --input_asr_path ${{inputs.input_asr_path}} $[[--event_type ${{inputs.event_type}}]] $[[--max_num_speakers ${{inputs.max_num_speakers}}]] --output_path ${{outputs.output_path}}